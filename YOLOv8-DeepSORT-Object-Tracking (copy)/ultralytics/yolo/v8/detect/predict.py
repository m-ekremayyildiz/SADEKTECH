# Ultralytics YOLO ğŸš€, GPL-3.0 license

import hydra # Hydra kÃ¼tÃ¼phanesini iÃ§eriye alÄ±r. Hydra, yapÄ±landÄ±rma yÃ¶netimi iÃ§in kullanÄ±lan bir kÃ¼tÃ¼phanedir. Bu, program parametrelerini ve yapÄ±landÄ±rmalarÄ±nÄ± kolayca yÃ¶netmek iÃ§in kullanÄ±lÄ±r.
import torch # PyTorch kÃ¼tÃ¼phanesini iÃ§eriye alÄ±r. PyTorch, derin Ã¶ÄŸrenme modelleri oluÅŸturmak ve eÄŸitmek iÃ§in kullanÄ±lan popÃ¼ler bir makine Ã¶ÄŸrenimi kÃ¼tÃ¼phanesidir.
import argparse # Komut satÄ±rÄ± argÃ¼manlarÄ±nÄ± iÅŸlemek iÃ§in kullanÄ±lan argparse kÃ¼tÃ¼phanesini iÃ§eriye alÄ±r. Program Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rken kullanÄ±cÄ±nÄ±n belirlediÄŸi parametreleri kolayca almak iÃ§in kullanÄ±lÄ±r.
import time # Zamanla ilgili iÅŸlemleri gerÃ§ekleÅŸtirmek iÃ§in kullanÄ±lan bir modÃ¼ldÃ¼r.
from pathlib import Path # Dosya yollarÄ±nÄ± temsil etmek ve iÅŸlemek iÃ§in kullanÄ±lan bir sÄ±nÄ±ftÄ±r. pathlib modÃ¼lÃ¼, dosya sistemleri Ã¼zerinde iÅŸlem yapmayÄ± kolaylaÅŸtÄ±ran bir araÃ§ saÄŸlar.

import cv2 # OpenCV (Open Source Computer Vision Library) kÃ¼tÃ¼phanesini iÃ§eriye alÄ±r. OpenCV, bilgisayar gÃ¶rÃ¼ÅŸÃ¼ ve bilgisayarla gÃ¶rme gÃ¶revleri iÃ§in bir dizi araÃ§ saÄŸlayan popÃ¼ler bir kÃ¼tÃ¼phanedir.
import torch 
import torch.backends.cudnn as cudnn # NVIDIA'nÄ±n Derin Sinir AÄŸÄ± KÃ¼tÃ¼phanesi'nin (cuDNN) PyTorch baÄŸlamÄ±nÄ± etkinleÅŸtirmek iÃ§in kullanÄ±lÄ±r. cuDNN, NVIDIA GPU'lar Ã¼zerinde hÄ±zlandÄ±rÄ±lmÄ±ÅŸ derin Ã¶ÄŸrenme iÅŸlemleri iÃ§in optimize edilmiÅŸ bir kÃ¼tÃ¼phanedir.
from numpy import random # Numpy kÃ¼tÃ¼phanesinin bir alt modÃ¼lÃ¼ olan random modÃ¼lÃ¼nÃ¼ iÃ§eriye alÄ±r. Bu, rastgele sayÄ± Ã¼retimi ve rastgele Ã¶ÄŸelerle Ã§alÄ±ÅŸma gibi rastgele iÅŸlemleri gerÃ§ekleÅŸtirmek iÃ§in kullanÄ±lÄ±r.
from ultralytics.yolo.engine.predictor import BasePredictor # predicctor.py dosyasÄ±ndan BasePredictor fonksiyonunu getiriyoruz.
from ultralytics.yolo.utils import DEFAULT_CONFIG, ROOT, ops
"""
    DEFAULT_CONFIG: Bu, YOLO (You Only Look Once) algoritmasÄ± iÃ§in varsayÄ±lan yapÄ±landÄ±rma ayarlarÄ±nÄ± iÃ§eren bir nesnedir. Bu ayarlar, modelin eÄŸitimi, tahmini ve diÄŸer iÅŸlemleri iÃ§in kullanÄ±lan parametreleri iÃ§erir.

    ROOT: Bu, YOLO'nun kÃ¶k dizinini temsil eden bir nesnedir. YOLO kÃ¼tÃ¼phanesinin Ã§alÄ±ÅŸma dizinini belirtir.

    ops: Bu, YOLO operasyonlarÄ±nÄ± (operations) iÃ§eren bir modÃ¼ldÃ¼r. YOLO'nun Ã§eÅŸitli iÅŸlevselliÄŸini uygulayan temel operasyonlarÄ± iÃ§erir.

Bu Ã¶ÄŸeler, YOLO modelini yapÄ±landÄ±rmak, Ã§alÄ±ÅŸtÄ±rmak ve sonuÃ§larÄ± iÅŸlemek iÃ§in kullanÄ±lÄ±r. DEFAULT_CONFIG Ã¶ÄŸesi, modelin varsayÄ±lan ayarlarÄ±nÄ± iÃ§erir ve 
bu ayarlar Ã¼zerinde Ã¶zelleÅŸtirmeler yaparak YOLO modelini belirli bir gÃ¶rev veya veri kÃ¼mesine uyarlamak mÃ¼mkÃ¼ndÃ¼r. 
ROOT ve ops Ã¶ÄŸeleri de ilgili kÃ¼tÃ¼phanenin alt bileÅŸenlerini kullanmak iÃ§in gereklidir.
"""
from ultralytics.yolo.utils.checks import check_imgsz # checks modÃ¼lÃ¼nden check_imgsz fonksiyonunu iÃ§eri alÄ±r. Bu fonksiyon, resim boyutlarÄ± gibi giriÅŸ parametrelerini kontrol etmek iÃ§in kullanÄ±labilir.
from ultralytics.yolo.utils.plotting import Annotator, colors, save_one_box # Bu, YOLO'nun sonuÃ§larÄ±nÄ± Ã§izim ve gÃ¶rselleÅŸtirme iÅŸlemleri iÃ§in gerekli olan plotting modÃ¼lÃ¼nden bazÄ± sÄ±nÄ±flarÄ± ve fonksiyonlarÄ± iÃ§eri alÄ±r. Annotator sÄ±nÄ±fÄ±, gÃ¶rÃ¼ntÃ¼ Ã¼zerine nesne tespiti sonuÃ§larÄ±nÄ± eklemek iÃ§in kullanÄ±labilir. colors ve save_one_box ise Ã§eÅŸitli renk ve kutu kaydetme iÅŸlemlerini iÃ§erir.

import cv2
import time
from deep_sort_pytorch.utils.parser import get_config # DeepSORT (Deep Simple Online and Realtime Tracking) algoritmasÄ± iÃ§in yapÄ±landÄ±rma ayarlarÄ±nÄ± okumak iÃ§in kullanÄ±lan get_config fonksiyonunu iÃ§eri alÄ±r.
from deep_sort_pytorch.deep_sort import DeepSort # DeepSORT algoritmasÄ±nÄ± iÃ§eren deep_sort modÃ¼lÃ¼nden DeepSort sÄ±nÄ±fÄ±nÄ± iÃ§eri alÄ±r. DeepSORT, nesne takibi iÃ§in kullanÄ±lan bir derin Ã¶ÄŸrenme tabanlÄ± algoritmadÄ±r.
from collections import deque # Python'Ä±n collections modÃ¼lÃ¼nden deque sÄ±nÄ±fÄ±nÄ± iÃ§eri alÄ±r. Bu, bir Ã§ift taraflÄ± kuyruk (double-ended queue) veri yapÄ±sÄ±nÄ± temsil eder ve genellikle belirli bir kapasiteye sahip bir veri geÃ§miÅŸini tutmak iÃ§in kullanÄ±lÄ±r.
import numpy as np # NumPy kÃ¼tÃ¼phanesini iÃ§eri alÄ±r. NumPy, Ã§ok boyutlu dizilerle Ã§alÄ±ÅŸmayÄ± saÄŸlayan ve bilimsel hesaplamalar iÃ§in kullanÄ±lan gÃ¼Ã§lÃ¼ bir Python kÃ¼tÃ¼phanesidir.
palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)
data_deque = {}

deepsort = None

def init_tracker(): # Bu fonksiyon, DeepSORT algoritmasÄ±nÄ±n baÅŸlatÄ±lmasÄ±nÄ± ve yapÄ±landÄ±rÄ±lmasÄ±nÄ± saÄŸlar. 
    global deepsort
    cfg_deep = get_config() # get_config fonksiyonunu kullanarak, DeepSORT algoritmasÄ±nÄ±n yapÄ±landÄ±rma ayarlarÄ±nÄ± iÃ§eren bir konfigÃ¼rasyon objesi (cfg_deep) oluÅŸturur.
    cfg_deep.merge_from_file("deep_sort_pytorch/configs/deep_sort.yaml") # YAML dosyasÄ±ndan gelen ek yapÄ±landÄ±rma ayarlarÄ±nÄ± cfg_deep objesine birleÅŸtirir. Bu dosya, DeepSORT'un Ã§eÅŸitli parametrelerini iÃ§erir ve algoritmanÄ±n davranÄ±ÅŸÄ±nÄ± ÅŸekillendirmek iÃ§in kullanÄ±lÄ±r.

    deepsort= DeepSort(cfg_deep.DEEPSORT.REID_CKPT, # Re-identification (ReID) modelinin checkpoint dosyasÄ±nÄ±n yolunu belirtir.
                            max_dist=cfg_deep.DEEPSORT.MAX_DIST, min_confidence=cfg_deep.DEEPSORT.MIN_CONFIDENCE, # Ä°ki takip noktasÄ± arasÄ±ndaki maksimum mesafeyi belirler. Minimum gÃ¼venilirlik deÄŸeri, bir nesnenin takip edilmesi iÃ§in bu deÄŸeri aÅŸmalÄ±dÄ±r.
                            nms_max_overlap=cfg_deep.DEEPSORT.NMS_MAX_OVERLAP, max_iou_distance=cfg_deep.DEEPSORT.MAX_IOU_DISTANCE, # Non-maximum suppression (NMS) iÃ§in maksimum Ã¶rtÃ¼ÅŸme eÅŸiÄŸi. Ä°ki nesnenin IOU (Intersection over Union) deÄŸerinin maksimum mesafesi.
                            max_age=cfg_deep.DEEPSORT.MAX_AGE, n_init=cfg_deep.DEEPSORT.N_INIT, nn_budget=cfg_deep.DEEPSORT.NN_BUDGET, # Bir nesnenin kaÃ§ Ã§erÃ§eve boyunca takip edileceÄŸini belirler. Bir nesnenin takibi iÃ§in kaÃ§ Ã§erÃ§eve boyunca algÄ±lama yapÄ±lacaÄŸÄ±nÄ± belirler. KullanÄ±lacak en yakÄ±n komÅŸu sayÄ±sÄ±nÄ± belirler.
                            use_cuda=True) # CUDA tabanlÄ± GPU kullanÄ±mÄ±nÄ± belirler (eÄŸer mevcutsa).
##########################################################################################
def xyxy_to_xywh(*xyxy):
    """ 
    Bu fonksiyonun parametresi, sÄ±nÄ±rlayÄ±cÄ± kutunun dÃ¶rt kÃ¶ÅŸe noktasÄ±nÄ±n koordinatlarÄ±nÄ± iÃ§eren bir demet (tuple) olarak kabul edilir.
    Fonksiyon, bu kÃ¶ÅŸe noktalarÄ±nÄ± kullanarak sÄ±nÄ±rlayÄ±cÄ± kutunun sol Ã¼st ve saÄŸ alt kÃ¶ÅŸelerinin koordinatlarÄ±nÄ± belirler. 
    
    Bu dÃ¶nÃ¼ÅŸÃ¼m, sÄ±nÄ±rlayÄ±cÄ± kutunun koordinatlarÄ±nÄ± merkez-gerÃ§ek geniÅŸlik (x_c, y_c, w, h) formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmek amacÄ±yla kullanÄ±lÄ±r. 
    Bu tÃ¼r bir format, Ã¶zellikle nesne tespiti ve takip algoritmalarÄ±nda sÄ±klÄ±kla kullanÄ±lÄ±r.
    """
    bbox_left = min([xyxy[0].item(), xyxy[2].item()])
    bbox_top = min([xyxy[1].item(), xyxy[3].item()])
    bbox_w = abs(xyxy[0].item() - xyxy[2].item())
    bbox_h = abs(xyxy[1].item() - xyxy[3].item())
    x_c = (bbox_left + bbox_w / 2)
    y_c = (bbox_top + bbox_h / 2)
    w = bbox_w
    h = bbox_h
    return x_c, y_c, w, h

def xyxy_to_tlwh(bbox_xyxy):
    """
    Bu fonksiyonun girdisi, sÄ±nÄ±rlayÄ±cÄ± kutularÄ±n koordinatlarÄ±nÄ± iÃ§eren bir liste olarak kabul edilir. 
    Her bir sÄ±nÄ±rlayÄ±cÄ± kutu, [x1, y1, x2, y2] formatÄ±nda dÃ¶rt kÃ¶ÅŸe noktasÄ±nÄ±n koordinatlarÄ±nÄ± iÃ§erir.

    Bu tÃ¼r bir dÃ¶nÃ¼ÅŸÃ¼m, Ã¶zellikle nesne takip (object tracking) uygulamalarÄ±nda kullanÄ±lÄ±r, 
    Ã§Ã¼nkÃ¼ bu tÃ¼r koordinatlar nesnenin Ã¼st sol kÃ¶ÅŸesinin konumunu ve geniÅŸlik-yÃ¼kseklik bilgilerini iÃ§erir.
    """
    tlwh_bboxs = []
    for i, box in enumerate(bbox_xyxy):
        x1, y1, x2, y2 = [int(i) for i in box]
        top = x1
        left = y1
        w = int(x2 - x1)
        h = int(y2 - y1)
        tlwh_obj = [top, left, w, h]
        tlwh_bboxs.append(tlwh_obj)
    return tlwh_bboxs

def compute_color_for_labels(label):
    """
    Bu fonksiyon, bir etiketin (label) sÄ±nÄ±fÄ±na baÄŸlÄ± olarak bir renk belirlemek iÃ§in kullanÄ±lÄ±r. Etiketlere Ã¶zel sabit renkler atanmÄ±ÅŸtÄ±r. 
    EÄŸer etiket, belirli bir sÄ±nÄ±fa karÅŸÄ±lÄ±k gelen sabit etiketlerden biriyle eÅŸleÅŸiyorsa, bu sÄ±nÄ±fa Ã¶zgÃ¼ olan sabit rengi dÃ¶ndÃ¼rÃ¼r. 
    EÄŸer etiket bu sabit sÄ±nÄ±flardan birine uymuyorsa, bir renk paletini kullanarak etikete Ã¶zel dinamik bir renk Ã¼retilir.
    """
    if label == 0: #person
        color = (85,45,255)
    elif label == 2: # Car
        color = (222,82,175)
    elif label == 3:  # Motobike
        color = (0, 204, 255)
    elif label == 5:  # Bus
        color = (0, 149, 255)
    else:
        color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]
    return tuple(color)

def draw_border(img, pt1, pt2, color, thickness, r, d): 
    """
    Bu fonksiyon, verilen koordinatlar arasÄ±nda bir dikdÃ¶rtgen Ã§izer ve bu dikdÃ¶rtgenin kÃ¶ÅŸelerine daireler ekler. 
    AyrÄ±ca, dikdÃ¶rtgenin kÃ¶ÅŸelerinden Ã§Ä±karak Ã§erÃ§eveyi sÃ¼slemek iÃ§in Ã§izgiler de ekler. Son olarak, dikdÃ¶rtgenin iÃ§ini ve 
    Ã§izgiler arasÄ±ndaki alanÄ± doldurarak Ã§erÃ§eveyi tamamlar. 
    Bu iÅŸlem, belirli bir nesnenin Ã§evresini vurgulamak veya gÃ¶rsel olarak Ã¶nemli bir alanÄ± belirtmek iÃ§in kullanÄ±labilir.
    """
    x1,y1 = pt1
    x2,y2 = pt2
    # Top left
    cv2.line(img, (x1 + r, y1), (x1 + r + d, y1), color, thickness)
    cv2.line(img, (x1, y1 + r), (x1, y1 + r + d), color, thickness)
    cv2.ellipse(img, (x1 + r, y1 + r), (r, r), 180, 0, 90, color, thickness)
    # Top right
    cv2.line(img, (x2 - r, y1), (x2 - r - d, y1), color, thickness)
    cv2.line(img, (x2, y1 + r), (x2, y1 + r + d), color, thickness)
    cv2.ellipse(img, (x2 - r, y1 + r), (r, r), 270, 0, 90, color, thickness)
    # Bottom left
    cv2.line(img, (x1 + r, y2), (x1 + r + d, y2), color, thickness)
    cv2.line(img, (x1, y2 - r), (x1, y2 - r - d), color, thickness)
    cv2.ellipse(img, (x1 + r, y2 - r), (r, r), 90, 0, 90, color, thickness)
    # Bottom right
    cv2.line(img, (x2 - r, y2), (x2 - r - d, y2), color, thickness)
    cv2.line(img, (x2, y2 - r), (x2, y2 - r - d), color, thickness)
    cv2.ellipse(img, (x2 - r, y2 - r), (r, r), 0, 0, 90, color, thickness)

    cv2.rectangle(img, (x1 + r, y1), (x2 - r, y2), color, -1, cv2.LINE_AA)
    cv2.rectangle(img, (x1, y1 + r), (x2, y2 - r - d), color, -1, cv2.LINE_AA)
    
    cv2.circle(img, (x1 +r, y1+r), 2, color, 12)
    cv2.circle(img, (x2 -r, y1+r), 2, color, 12)
    cv2.circle(img, (x1 +r, y2-r), 2, color, 12)
    cv2.circle(img, (x2 -r, y2-r), 2, color, 12)
    
    return img

def UI_box(x, img, color=None, label=None, line_thickness=None):
    """
    Bu fonksiyon, bir nesnenin sÄ±nÄ±rlayÄ±cÄ± kutusunu (bounding box) bir gÃ¶rÃ¼ntÃ¼ Ã¼zerinde Ã§izmek ve bu kutuya etiket (label) eklemek iÃ§in kullanÄ±lÄ±r. Fonksiyonun parametreleri ÅŸu ÅŸekildedir:

    x: Bounding box koordinatlarÄ± (sol Ã¼st ve saÄŸ alt kÃ¶ÅŸe koordinatlarÄ±).
    img: Bounding box'un Ã§izileceÄŸi gÃ¶rÃ¼ntÃ¼.
    color: Bounding box ve etiketin renkleri. VarsayÄ±lan olarak rastgele bir renk seÃ§ilir.
    label: Bounding box Ã¼zerinde gÃ¶rÃ¼ntÃ¼lenecek etiket.
    line_thickness: Ã‡izgi ve font kalÄ±nlÄ±ÄŸÄ±. VarsayÄ±lan olarak gÃ¶rÃ¼ntÃ¼nÃ¼n boyutlarÄ±na baÄŸlÄ± olarak dinamik bir deÄŸer atanÄ±r.

    Fonksiyon, cv2.rectangle iÅŸlevini kullanarak bounding box'u Ã§izer. 
    ArdÄ±ndan, eÄŸer bir etiket (label) belirtilmiÅŸse, etiketi Ã§izilen bounding box'un sol Ã¼st kÃ¶ÅŸesine ekler. 
    Etiketin Ã¼zerine Ã§izilen Ã§izgi ve elips detaylarÄ±, draw_border adlÄ± baÅŸka bir fonksiyon kullanÄ±larak eklenir. 
    Bu ÅŸekilde, nesnelerin gÃ¶rÃ¼ntÃ¼ Ã¼zerinde belirgin bir ÅŸekilde iÅŸaretlenmesi ve izlenmesi saÄŸlanÄ±r.
    """
    # Plots one bounding box on image img
    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness
    color = color or [random.randint(0, 255) for _ in range(3)]
    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))
    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)
    if label:
        tf = max(tl - 1, 1)  # font thickness
        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]

        img = draw_border(img, (c1[0], c1[1] - t_size[1] -3), (c1[0] + t_size[0], c1[1]+3), color, 1, 8, 2)

        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)



def draw_boxes(img, bbox, names,object_id, identities=None, offset=(0, 0)):
    """
    Bu fonksiyon, nesnelerin (Ã¶rnek olarak araÃ§ veya insan) takibini gerÃ§ekleÅŸtiren bir nesne takip algoritmasÄ± sonuÃ§larÄ±nÄ± gÃ¶rÃ¼ntÃ¼ Ã¼zerinde gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lÄ±r. Kodun ana amaÃ§larÄ± ÅŸunlardÄ±r:

    draw_boxes fonksiyonu, nesne takip algoritmasÄ±nÄ±n Ã§Ä±kÄ±ÅŸÄ± olan bounding box'larÄ± (sÄ±nÄ±rlayÄ±cÄ± kutularÄ±) ve bunlara ait kimlik bilgilerini gÃ¶rÃ¼ntÃ¼ Ã¼zerinde Ã§izer.
    GÃ¶rÃ¼ntÃ¼ Ã¼zerinde her bir nesnenin etrafÄ±na bir sÄ±nÄ±rlayÄ±cÄ± kutu (bounding box) Ã§izilir ve bu kutulara nesnenin kimliÄŸi ve adÄ± eklenir.
    Her nesnenin takip edilen yolunu gÃ¶stermek iÃ§in geÃ§miÅŸ konumlarÄ±ndan oluÅŸan bir izleme Ã§izgisi Ã§izilir. Bu izleme Ã§izgileri, nesnenin geÃ§miÅŸ konumlarÄ±na dayanarak dinamik bir kalÄ±nlÄ±kla Ã§izilir, 
    yani nesne uzun sÃ¼re takip edildiyse Ã§izgi kalÄ±n, kÄ±sa sÃ¼re takip edildiyse ince olur.
    Nesnelerin takip edildiÄŸi sÃ¼re boyunca gÃ¶rÃ¼ntÃ¼ Ã¼zerindeki buffer'da (veri deposu) bu nesnelere ait konum bilgileri saklanÄ±r ve her bir frame'de bu konumlar gÃ¼ncellenir.

    Bu kod, nesne takibi sÄ±rasÄ±nda nesnelerin hareketini ve takip edilen yollarÄ±nÄ± gÃ¶rselleÅŸtirerek, takip sÃ¼recini daha anlaÅŸÄ±lÄ±r hale getirmeyi amaÃ§lar.
    """
    #cv2.line(img, line[0], line[1], (46,162,112), 3)

    height, width, _ = img.shape
    # remove tracked point from buffer if object is lost
    for key in list(data_deque):
      if key not in identities:
        data_deque.pop(key)

    for i, box in enumerate(bbox):
        x1, y1, x2, y2 = [int(i) for i in box]
        x1 += offset[0]
        x2 += offset[0]
        y1 += offset[1]
        y2 += offset[1]

        # code to find center of bottom edge
        center = (int((x2+x1)/ 2), int((y2+y2)/2))

        # get ID of object
        id = int(identities[i]) if identities is not None else 0

        # create new buffer for new object
        if id not in data_deque:  
          data_deque[id] = deque(maxlen= 64)
        color = compute_color_for_labels(object_id[i])
        obj_name = names[object_id[i]]
        label = '{}{:d}'.format("", id) + ":"+ '%s' % (obj_name)

        # add center to buffer
        data_deque[id].appendleft(center)
        UI_box(box, img, label=label, color=color, line_thickness=2)
        # draw trail
        for i in range(1, len(data_deque[id])):
            # check if on buffer value is none
            if data_deque[id][i - 1] is None or data_deque[id][i] is None:
                continue
            # generate dynamic thickness of trails
            thickness = int(np.sqrt(64 / float(i + i)) * 1.5)
            # draw trails
            cv2.line(img, data_deque[id][i - 1], data_deque[id][i], color, thickness)
    return img


class DetectionPredictor(BasePredictor):
    """
    nesne tespiti (object detection) ve nesne takibi (object tracking) yapabilen bir modeli kullanarak, 
    girdi gÃ¶rÃ¼ntÃ¼ler Ã¼zerinde nesneleri tespit etmeyi ve takip etmeyi saÄŸlar. 
    Kodun temel amacÄ± ÅŸu adÄ±mlarÄ± gerÃ§ekleÅŸtirmektir:
    """

    def get_annotator(self, img):
        
        return Annotator(img, line_width=self.args.line_thickness, example=str(self.model.names))

    def preprocess(self, img):
        """
        preprocess fonksiyonu, giriÅŸ gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ modelin girdi formatÄ±na uygun hale getirir. 
        GiriÅŸ gÃ¶rÃ¼ntÃ¼sÃ¼, Ã¶nce PyTorch tensor'una dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r, ardÄ±ndan eÄŸer modelin iÅŸlemesi gereken formatta deÄŸilse uygun formata Ã§evrilir. 
        Ã–zellikle, gÃ¶rÃ¼ntÃ¼ pikselleri 0-255 aralÄ±ÄŸÄ±ndan 0.0-1.0 aralÄ±ÄŸÄ±na normalize edilir.
        """
        img = torch.from_numpy(img).to(self.model.device)
        img = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32
        img /= 255  # 0 - 255 to 0.0 - 1.0
        return img

    def postprocess(self, preds, img, orig_img):
        """
        postprocess fonksiyonu, modelin Ã§Ä±ktÄ±larÄ± olan tespit sonuÃ§larÄ±nÄ± alÄ±r. 
        Bu tespit sonuÃ§larÄ±, non_max_suppression fonksiyonu ile aynÄ± sÄ±nÄ±fa ait ve birbirine yakÄ±n olan tespitler arasÄ±nda bir eleme yaparak dÃ¼zenlenir. 
        ArdÄ±ndan, tespit sonuÃ§larÄ±, orijinal gÃ¶rÃ¼ntÃ¼ boyutlarÄ±na gÃ¶re Ã¶lÃ§eklenir ve yuvarlanÄ±r (rounding).
        """
        preds = ops.non_max_suppression(preds,
                                        self.args.conf,
                                        self.args.iou,
                                        agnostic=self.args.agnostic_nms,
                                        max_det=self.args.max_det)

        for i, pred in enumerate(preds):
            shape = orig_img[i].shape if self.webcam else orig_img.shape
            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], shape).round()

        return preds

    def write_results(self, idx, preds, batch):
        start_time = time.time()
        """
        write_results fonksiyonu, nesne tespiti sonuÃ§larÄ±nÄ± yazmak ve gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lÄ±r. 
        Tespit sonuÃ§larÄ±, orijinal gÃ¶rÃ¼ntÃ¼ Ã¼zerine Ã§izilir ve nesnelerin etrafÄ±na sÄ±nÄ±rlayÄ±cÄ± kutular (bounding box) eklenir.
        AyrÄ±ca, her sÄ±nÄ±ftaki nesne sayÄ±sÄ± ve isimleri yazÄ±larak Ã§Ä±ktÄ± olarak dÃ¶ndÃ¼rÃ¼lÃ¼r.
        """
        p, im, im0 = batch
        all_outputs = []
        log_string = ""
        if len(im.shape) == 3:
            im = im[None]  # expand for batch dim
        self.seen += 1
        im0 = im0.copy()
        if self.webcam:  # batch_size >= 1
            log_string += f'{idx}: ' # 0: kÄ±smÄ±
            frame = self.dataset.count
        else:
            frame = getattr(self.dataset, 'frame', 0)

        self.data_path = p
        save_path = str(self.save_dir / p.name)  # im.jpg
        self.txt_path = str(self.save_dir / 'labels' / p.stem) + ('' if self.dataset.mode == 'image' else f'_{frame}')
        log_string += '%gx%g ' % im.shape[2:]  # print string 480x640 gibi
        log_string += str(start_time)
        self.annotator = self.get_annotator(im0)
        det = preds[idx]
        all_outputs.append(det)
        if len(det) == 0:
            return log_string
       
        for c in det[:, 5].unique():
            n = (det[:, 5] == c).sum()  # detections per class
            #log_string += f"{n} {self.model.names[int(c)]}{'s' * (n > 1)}, "
        # write
        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
        xywh_bboxs = []
        confs = []
        oids = []
        outputs = []
       
        for *xyxy, conf, cls in reversed(det):
            x_c, y_c, bbox_w, bbox_h = xyxy_to_xywh(*xyxy)
            xywh_obj = [x_c, y_c, bbox_w, bbox_h]
            xywh_bboxs.append(xywh_obj)
            confs.append([conf.item()])
            oids.append(int(cls))
        xywhs = torch.Tensor(xywh_bboxs)
        confss = torch.Tensor(confs)
          
        outputs = deepsort.update(xywhs, confss, oids, im0)
        if len(outputs) > 0:
            bbox_xyxy = outputs[:, :4]
            identities = outputs[:, -2]
            object_id = outputs[:, -1]
            
            draw_boxes(im0, bbox_xyxy, self.model.names, object_id,identities)
   
        return log_string



"""
@hydra.main decorator'Ä±, Hydra kÃ¼tÃ¼phanesini kullanarak yapÄ±landÄ±rma dosyalarÄ±nÄ± iÅŸlemek iÃ§in kullanÄ±lÄ±r. 
Hydra, yapÄ±landÄ±rma dosyalarÄ± aracÄ±lÄ±ÄŸÄ±yla program parametrelerini yÃ¶netmeyi saÄŸlar. 
Bu sayede kodu Ã§alÄ±ÅŸtÄ±rÄ±rken farklÄ± parametre setlerini kolayca belirleyebilirsiniz.
""" 
@hydra.main(version_base=None, config_path=str(DEFAULT_CONFIG.parent), config_name=DEFAULT_CONFIG.name) 
def predict(cfg): # predict fonksiyonu, nesne tespiti ve takibi iÅŸlemlerini gerÃ§ekleÅŸtirir.
    init_tracker() # init_tracker() fonksiyonu, nesne takibi iÃ§in gerekli olan deepsort adlÄ± bir takipÃ§i (tracker) modelini baÅŸlatÄ±r.
    cfg.model = cfg.model or "yolov8n.pt" # cfg.model ifadesi, kullanÄ±lacak olan nesne tespiti modelinin dosya yolunu belirler. EÄŸer cfg.model deÄŸeri belirtilmemiÅŸse veya None ise, varsayÄ±lan bir model olan "yolov8n.pt" kullanÄ±lÄ±r.
    cfg.imgsz = check_imgsz(cfg.imgsz, min_dim=2)  # check image size, cfg.imgsz ifadesi, nesne tespiti modelinin iÅŸleyeceÄŸi gÃ¶rÃ¼ntÃ¼lerin boyutunu belirler. Bu deÄŸer, check_imgsz fonksiyonu aracÄ±lÄ±ÄŸÄ±yla kontrol edilir ve minimum boyut 2 olarak ayarlanÄ±r.
    cfg.source = cfg.source if cfg.source is not None else ROOT / "assets" # cfg.source ifadesi, nesne tespiti ve takibi modeline veri saÄŸlayacak olan kaynaÄŸÄ± belirler. EÄŸer bu ifade belirtilmemiÅŸse veya None ise, varsayÄ±lan olarak ROOT / "assets" (muhtemelen proje kÃ¶k dizini altÄ±ndaki "assets" klasÃ¶rÃ¼) kullanÄ±lÄ±r.
    predictor = DetectionPredictor(cfg) # predictor deÄŸiÅŸkeni, DetectionPredictor sÄ±nÄ±fÄ±ndan bir Ã¶rnektir. Bu Ã¶rnek, nesne tespiti ve takibi modelini Ã§alÄ±ÅŸtÄ±rmak iÃ§in gerekli olan parametreleri alÄ±r.
    print(predictor)
    predictor() # predictor() ifadesi, DetectionPredictor sÄ±nÄ±fÄ±ndaki __call__ metodunu Ã§aÄŸÄ±rarak nesne tespiti ve takibi iÅŸlemlerini baÅŸlatÄ±r. Bu metod, gÃ¶rÃ¼ntÃ¼lerin iÅŸlenmesi, nesne tespiti, takip ve sonuÃ§larÄ±n gÃ¶rselleÅŸtirilmesi gibi adÄ±mlarÄ± iÃ§erir.


if __name__ == "__main__":
    predict()
    cv2.destroyAllWindows()  # TÃ¼m OpenCV pencerelerini kapat


